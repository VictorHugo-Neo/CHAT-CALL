# ü§ñ ChatCall AI

> **‚ö†Ô∏è Status do Projeto: Em Desenvolvimento ‚ö†Ô∏è**
>
> Este projeto √© um trabalho em progresso. As funcionalidades principais de chat e salvamento de hist√≥rico est√£o implementadas, mas novos recursos (listados no Roadmap) est√£o sendo ativamente desenvolvidos.

Bem-vindo ao ChatCall AI, um aplicativo de chat full-stack que se conecta a um modelo de IA local (via Ollama) para conversas em tempo real. Todo o hist√≥rico da conversa √© salvo e carregado a partir de um banco de dados PostgreSQL.

Este projeto serve como um *template* completo para construir aplica√ß√µes de IA usando o "stack" FastAPI (Python) e React (TypeScript).

---

## üé® Funcionalidades Atuais

* **Chat em Tempo Real:** Converse com o modelo de IA **Mistral** (ou outro) rodando localmente via Ollama.
* **Backend em FastAPI:** Uma API robusta em Python para servir como c√©rebro da aplica√ß√£o.
* **Frontend em React (Vite):** Uma interface de usu√°rio moderna, r√°pida e reativa, constru√≠da com TypeScript.
* **Persist√™ncia de Dados:** O hist√≥rico completo do chat √© salvo em um banco PostgreSQL.
* **Carregamento de Hist√≥rico:** Ao abrir o app, o hist√≥rico de chat anterior √© carregado automaticamente.
* **UI Limpa:** Estiliza√ß√£o feita com TailwindCSS.
* **Gerenciamento de Estado:** O estado do frontend √© gerenciado de forma limpa usando React Context.

---

## üó∫Ô∏è Roadmap (Funcionalidades Planejadas)

O futuro deste projeto inclui:

* **M√∫ltiplos Chats:** Permitir que o usu√°rio crie e gerencie v√°rias sess√µes de chat separadas (ex: "Chat sobre Python", "Chat sobre Hist√≥ria").
* **Chat com Documentos (RAG):** Capacidade de fazer upload de um PDF e conversar com a IA sobre o conte√∫do espec√≠fico daquele documento.
* **Gerenciamento de Conversas:** Funcionalidades para renomear e apagar chats salvos.
* **Autentica√ß√£o de Usu√°rio:** (Opcional) Sistema de login para salvar chats na nuvem.

---

## üõ†Ô∏è Stack de Tecnologias

Este projeto √© um "monorepo" dividido em duas partes principais:

### Backend (`/backend`)
* **Python 3.12+**
* **FastAPI:** Para a cria√ß√£o da API REST.
* **LangChain:** Para se comunicar com o modelo de linguagem.
* **Ollama:** Para rodar os modelos de LLM localmente.
* **PostgreSQL:** Como banco de dados para salvar o hist√≥rico.
* **SQLAlchemy:** Como ORM para se comunicar com o PostgreSQL.

### Frontend (`/frontend`)
* **Vite:** Para "buildar" o projeto React.
* **React:** Para a constru√ß√£o da interface de usu√°rio.
* **TypeScript:** Para adicionar tipagem est√°tica ao c√≥digo.
* **TailwindCSS:** Para estiliza√ß√£o da UI.
* **React Context:** Para gerenciamento de estado global.

---

## ‚öôÔ∏è Configura√ß√£o de Vari√°veis de Ambiente (.env)

Este projeto usa arquivos `.env` para gerenciar segredos e configura√ß√µes. Voc√™ **deve** cri√°-los antes de rodar.

### 1. Backend (`/backend/.env`)

O backend precisa de um arquivo `.env` para se conectar ao banco de dados PostgreSQL.

1.  Na pasta `/backend`, crie um novo arquivo chamado `.env`.
2.  Adicione a sua string de conex√£o do banco:

    ```ini
    # /backend/.env
    DATABASE_URL="postgresql://SEU_USUARIO:SUA_SENHA@localhost/chat_call_db"
    ```
    *(Substitua `SEU_USUARIO`, `SUA_SENHA` e `chat_call_db` pelos seus dados reais do PostgreSQL.)*

### 2. Frontend (`/frontend/.env`)

O frontend precisa de um arquivo `.env` para saber onde o backend est√° rodando.

1.  Na pasta `/frontend`, crie um novo arquivo chamado `.env`.
2.  Adicione a URL base da sua API FastAPI:

    ```ini
    # /frontend/.env
    VITE_API_BASE_URL="[http://127.0.0.1:8000/api/v1](http://127.0.0.1:8000/api/v1)"
    ```
    *(**Importante:** No Vite, todas as vari√°veis de ambiente expostas ao navegador **devem** come√ßar com o prefixo `VITE_`.)*

---

## üöÄ Como Rodar o Projeto

Para rodar este projeto, voc√™ precisar√° de **tr√™s** terminais: um para o Backend, um para o Frontend e um para garantir que o Ollama esteja rodando.

### Pr√©-requisitos
* **Git**
* **Python 3.10+** (e `pip`)
* **Node.js v18+** (e `npm`)
* **Ollama:** [Baixe e instale o Ollama](https://ollama.com/).
* **Um Modelo de IA:** Ap√≥s instalar o Ollama, puxe o Mistral: `ollama pull mistral`
* **PostgreSQL:** Um servidor PostgreSQL rodando localmente.

---

### 1. Configura√ß√£o do Backend (Terminal 1)

1.  **Crie seu Banco de Dados:**
    * No seu PostgreSQL, crie um novo banco de dados (ex: `CREATE DATABASE chat_call_db;`).
    
2.  **Navegue at√© a pasta do backend:**
    ```bash
    cd backend
    ```

3.  **Crie e ative um ambiente virtual:**
    ```bash
    # Windows
    python -m venv venv
    .\venv\Scripts\activate
    
    # macOS/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

4.  **Instale as depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

5.  **Crie as Tabelas no Banco:**
    (Ap√≥s ter configurado o `.env` do backend, rode este comando uma vez)
    ```bash
    python create_tables.py
    ```

6.  **Inicie o Servidor Backend:**
    ```bash
    uvicorn app.main:app --reload
    ```
    * O servidor estar√° rodando em `http://127.0.0.1:8000`

---

### 2. Configura√ß√£o do Frontend (Terminal 2)

1.  **Abra um *novo terminal*** e navegue at√© a pasta do frontend:
    ```bash
    cd frontend
    ```

2.  **Instale as depend√™ncias:**
    ```bash
    npm install
    ```

3.  **Inicie o Servidor de Desenvolvimento:**
    (Isso ler√° seu arquivo `.env` do frontend)
    ```bash
    npm run dev
    ```
    * O servidor estar√° rodando em `http://localhost:5173` (ou outra porta indicada).

---

### 3. Garanta que o Ollama esteja Rodando

1.  Inicie o aplicativo **Ollama** no seu computador (ele deve aparecer na barra de tarefas).
2.  Certifique-se de que o modelo "mistral" est√° dispon√≠vel.

### 4. Use o Aplicativo!
Abra `http://localhost:5173` no seu navegador. O frontend carregar√° o hist√≥rico e voc√™ poder√° come√ßar a conversar.